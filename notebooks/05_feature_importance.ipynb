{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feature Importance\n\nThis notebook contains the code from `feature_importance.py`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\nFeature Importance Analysis Module\n\nThis module analyzes feature importance across all models:\n1. Random Forest: Direct feature importances\n2. Logistic Regression: Coefficient analysis\n3. Deep Learning: Permutation importance\n4. Comparative analysis and visualizations\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Model loading and analysis\nimport joblib\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.metrics import accuracy_score\n\ntry:\n    import tensorflow as tf\n    from tensorflow import keras\n    DL_AVAILABLE = True\nexcept ImportError:\n    DL_AVAILABLE = False\n\n\ndef load_data_and_models(model_dir='../models', features_file='../data/processed/arsenal_features.csv'):\n    \"\"\"\n    Load data and all trained models.\n    \n    Returns:\n        dict: Contains data and models\n    \"\"\"\n    print(\"=\" * 60)\n    print(\"LOADING DATA AND MODELS\")\n    print(\"=\" * 60)\n    \n    # Load features\n    df = pd.read_csv(features_file)\n    df['Date'] = pd.to_datetime(df['Date'])\n    df = df.sort_values('Date').reset_index(drop=True)\n    \n    # Separate features and target\n    feature_cols = [col for col in df.columns \n                   if col not in ['Result', 'Season', 'Date', 'Opponent']]\n    X = df[feature_cols].copy()\n    y = df['Result'].copy()\n    \n    # Handle missing values\n    X = X.fillna(X.median())\n    X = X.replace([np.inf, -np.inf], 0)\n    \n    # Drop opponent_league_position if exists (models trained without it)\n    if 'opponent_league_position' in X.columns:\n        X = X.drop(columns=['opponent_league_position'])\n        feature_cols = [col for col in feature_cols if col != 'opponent_league_position']\n    \n    # Train/test split\n    train_mask = df['Season'] != '2024-25'\n    test_mask = df['Season'] == '2024-25'\n    \n    X_train = X[train_mask].copy()\n    y_train = y[train_mask].copy()\n    X_test = X[test_mask].copy()\n    y_test = y[test_mask].copy()\n    \n    print(f\"\\nTraining set: {len(X_train):,} samples\")\n    print(f\"Test set: {len(X_test):,} samples\")\n    print(f\"Features: {len(feature_cols)}\")\n    \n    # Load models\n    models = {}\n    model_path = Path(model_dir)\n    \n    # Random Forest\n    try:\n        rf_data = joblib.load(model_path / 'random_forest.pkl')\n        models['random_forest'] = {\n            'model': rf_data['model'],\n            'label_encoder': rf_data['label_encoder'],\n            'X_train': X_train,\n            'X_test': X_test,\n            'y_train': y_train,\n            'y_test': y_test,\n            'feature_names': X_train.columns.tolist()\n        }\n        print(\"  \u2713 Loaded Random Forest\")\n    except Exception as e:\n        print(f\"  \u2717 Random Forest: {e}\")\n    \n    # Logistic Regression\n    try:\n        lr_data = joblib.load(model_path / 'logreg_baseline.pkl')\n        models['logistic_regression'] = {\n            'model': lr_data['model'],\n            'scaler': lr_data['scaler'],\n            'label_encoder': lr_data['label_encoder'],\n            'X_train': X_train,\n            'X_test': X_test,\n            'y_train': y_train,\n            'y_test': y_test,\n            'feature_names': X_train.columns.tolist()\n        }\n        print(\"  \u2713 Loaded Logistic Regression\")\n    except Exception as e:\n        print(f\"  \u2717 Logistic Regression: {e}\")\n    \n    # Neural Network (with class weights - best model)\n    if DL_AVAILABLE:\n        try:\n            nn_model = keras.models.load_model(model_path / 'neural_network_weighted.h5')\n            nn_preprocess = joblib.load(model_path / 'neural_network_weighted_preprocessing.pkl')\n            scaler = nn_preprocess['scaler']\n            \n            # Align features\n            X_train_nn = X_train.copy()\n            X_test_nn = X_test.copy()\n            if hasattr(scaler, 'feature_names_in_'):\n                X_train_nn = X_train_nn[scaler.feature_names_in_]\n                X_test_nn = X_test_nn[scaler.feature_names_in_]\n            \n            X_train_scaled = scaler.transform(X_train_nn)\n            X_test_scaled = scaler.transform(X_test_nn)\n            \n            models['neural_network'] = {\n                'model': nn_model,\n                'scaler': scaler,\n                'label_encoder': nn_preprocess['label_encoder'],\n                'X_train': X_train_scaled,\n                'X_test': X_test_scaled,\n                'y_train': y_train,\n                'y_test': y_test,\n                'feature_names': X_train_nn.columns.tolist() if hasattr(X_train_nn, 'columns') else list(range(X_train_scaled.shape[1]))\n            }\n            print(\"  \u2713 Loaded Neural Network (Class Weights)\")\n        except Exception as e:\n            print(f\"  \u2717 Neural Network: {e}\")\n    \n    return models, feature_cols\n\n\ndef analyze_random_forest_importance(model_data, output_dir='../figures'):\n    \"\"\"\n    Analyze feature importance for Random Forest.\n    \n    Args:\n        model_data: Dictionary with model and data\n        output_dir: Directory to save plots\n    \"\"\"\n    print(\"\\n\" + \"=\" * 60)\n    print(\"RANDOM FOREST - FEATURE IMPORTANCE\")\n    print(\"=\" * 60)\n    \n    model = model_data['model']\n    feature_names = model_data['feature_names']\n    \n    # Get feature importances\n    importances = model.feature_importances_\n    \n    # Create DataFrame\n    importance_df = pd.DataFrame({\n        'Feature': feature_names,\n        'Importance': importances\n    }).sort_values('Importance', ascending=False)\n    \n    print(\"\\nTop 15 Most Important Features:\")\n    print(importance_df.head(15).to_string(index=False))\n    \n    # Plot\n    plt.figure(figsize=(12, 8))\n    top_features = importance_df.head(20)\n    plt.barh(range(len(top_features)), top_features['Importance'].values)\n    plt.yticks(range(len(top_features)), top_features['Feature'].values)\n    plt.xlabel('Feature Importance', fontsize=12)\n    plt.title('Random Forest - Top 20 Feature Importances', fontsize=14, fontweight='bold')\n    plt.gca().invert_yaxis()\n    plt.tight_layout()\n    \n    output_path = Path(output_dir)\n    output_path.mkdir(parents=True, exist_ok=True)\n    plt.savefig(output_path / 'random_forest_feature_importance.png', dpi=300, bbox_inches='tight')\n    print(f\"\\n\u2713 Saved: {output_path / 'random_forest_feature_importance.png'}\")\n    plt.close()\n    \n    return importance_df\n\n\ndef analyze_logistic_regression_coefficients(model_data, output_dir='../figures'):\n    \"\"\"\n    Analyze coefficients for Logistic Regression.\n    \n    Args:\n        model_data: Dictionary with model and data\n        output_dir: Directory to save plots\n    \"\"\"\n    print(\"\\n\" + \"=\" * 60)\n    print(\"LOGISTIC REGRESSION - COEFFICIENT ANALYSIS\")\n    print(\"=\" * 60)\n    \n    model = model_data['model']\n    le = model_data['label_encoder']\n    feature_names = model_data['feature_names']\n    \n    # Get coefficients for each class\n    coefficients = model.coef_  # Shape: (n_classes, n_features)\n    class_names = le.classes_\n    \n    # Create DataFrame for each class\n    all_coefs = []\n    for i, class_name in enumerate(class_names):\n        coef_df = pd.DataFrame({\n            'Feature': feature_names,\n            'Coefficient': coefficients[i],\n            'Class': class_name\n        }).sort_values('Coefficient', key=abs, ascending=False)\n        all_coefs.append(coef_df)\n        \n        print(f\"\\n{class_name} - Top 10 Features (by absolute coefficient):\")\n        print(coef_df.head(10)[['Feature', 'Coefficient']].to_string(index=False))\n    \n    # Plot coefficients for each class\n    fig, axes = plt.subplots(1, len(class_names), figsize=(18, 6))\n    if len(class_names) == 1:\n        axes = [axes]\n    \n    for i, (ax, class_name, coef_df) in enumerate(zip(axes, class_names, all_coefs)):\n        top_coefs = coef_df.head(15)\n        colors = ['green' if x > 0 else 'red' for x in top_coefs['Coefficient'].values]\n        ax.barh(range(len(top_coefs)), top_coefs['Coefficient'].values, color=colors)\n        ax.set_yticks(range(len(top_coefs)))\n        ax.set_yticklabels(top_coefs['Feature'].values)\n        ax.set_xlabel('Coefficient Value', fontsize=11)\n        ax.set_title(f'{class_name} - Top 15 Features', fontsize=12, fontweight='bold')\n        ax.axvline(x=0, color='black', linestyle='--', linewidth=0.8)\n        ax.invert_yaxis()\n        ax.grid(True, alpha=0.3, axis='x')\n    \n    plt.suptitle('Logistic Regression - Feature Coefficients by Class', \n                 fontsize=14, fontweight='bold', y=1.02)\n    plt.tight_layout()\n    \n    output_path = Path(output_dir)\n    output_path.mkdir(parents=True, exist_ok=True)\n    plt.savefig(output_path / 'logistic_regression_coefficients.png', dpi=300, bbox_inches='tight')\n    print(f\"\\n\u2713 Saved: {output_path / 'logistic_regression_coefficients.png'}\")\n    plt.close()\n    \n    return all_coefs\n\n\ndef analyze_neural_network_permutation_importance(model_data, output_dir='../figures', n_repeats=10):\n    \"\"\"\n    Analyze permutation importance for Neural Network.\n    \n    Args:\n        model_data: Dictionary with model and data\n        output_dir: Directory to save plots\n        n_repeats: Number of times to permute each feature\n    \"\"\"\n    print(\"\\n\" + \"=\" * 60)\n    print(\"NEURAL NETWORK - PERMUTATION IMPORTANCE\")\n    print(\"=\" * 60)\n    \n    model = model_data['model']\n    X_test = model_data['X_test']\n    y_test = model_data['y_test']\n    le = model_data['label_encoder']\n    feature_names = model_data['feature_names']\n    \n    # Encode labels\n    y_test_encoded = le.transform(y_test)\n    \n    # Get baseline score\n    y_pred_proba = model.predict(X_test, verbose=0)\n    y_pred = np.argmax(y_pred_proba, axis=1)\n    baseline_score = accuracy_score(y_test_encoded, y_pred)\n    \n    print(f\"\\nBaseline accuracy: {baseline_score:.4f}\")\n    print(f\"Calculating permutation importance (n_repeats={n_repeats})...\")\n    print(\"  This may take a few minutes...\")\n    \n    # Manual permutation importance calculation (more reliable for Keras models)\n    n_features = X_test.shape[1]\n    importances = np.zeros(n_features)\n    importances_std = np.zeros(n_features)\n    \n    for i in range(n_features):\n        scores = []\n        for _ in range(n_repeats):\n            # Create a copy and permute the feature\n            X_test_permuted = X_test.copy()\n            np.random.shuffle(X_test_permuted[:, i])\n            \n            # Predict with permuted feature\n            y_pred_proba = model.predict(X_test_permuted, verbose=0)\n            y_pred = np.argmax(y_pred_proba, axis=1)\n            score = accuracy_score(y_test_encoded, y_pred)\n            scores.append(score)\n        \n        # Importance is the decrease in score\n        importances[i] = baseline_score - np.mean(scores)\n        importances_std[i] = np.std(scores)\n        \n        if (i + 1) % 5 == 0:\n            print(f\"  Processed {i + 1}/{n_features} features...\")\n    \n    # Create DataFrame\n    importance_df = pd.DataFrame({\n        'Feature': feature_names,\n        'Importance_Mean': importances,\n        'Importance_Std': importances_std\n    }).sort_values('Importance_Mean', ascending=False)\n    \n    print(\"\\nTop 15 Most Important Features:\")\n    print(importance_df.head(15).to_string(index=False))\n    \n    # Plot\n    plt.figure(figsize=(12, 8))\n    top_features = importance_df.head(20)\n    y_pos = np.arange(len(top_features))\n    plt.barh(y_pos, top_features['Importance_Mean'].values, \n             xerr=top_features['Importance_Std'].values, capsize=3)\n    plt.yticks(y_pos, top_features['Feature'].values)\n    plt.xlabel('Permutation Importance (Accuracy Decrease)', fontsize=12)\n    plt.title('Neural Network - Top 20 Feature Importances (Permutation)', \n              fontsize=14, fontweight='bold')\n    plt.gca().invert_yaxis()\n    plt.tight_layout()\n    \n    output_path = Path(output_dir)\n    output_path.mkdir(parents=True, exist_ok=True)\n    plt.savefig(output_path / 'neural_network_permutation_importance.png', dpi=300, bbox_inches='tight')\n    print(f\"\\n\u2713 Saved: {output_path / 'neural_network_permutation_importance.png'}\")\n    plt.close()\n    \n    return importance_df\n\n\ndef compare_feature_importance_across_models(models, output_dir='../figures'):\n    \"\"\"\n    Compare feature importance across all models.\n    \n    Args:\n        models: Dictionary of all models\n        output_dir: Directory to save plots\n    \"\"\"\n    print(\"\\n\" + \"=\" * 60)\n    print(\"COMPARATIVE FEATURE IMPORTANCE\")\n    print(\"=\" * 60)\n    \n    all_importances = {}\n    \n    # Random Forest\n    if 'random_forest' in models:\n        rf_importance = analyze_random_forest_importance(models['random_forest'], output_dir)\n        all_importances['Random Forest'] = rf_importance.set_index('Feature')['Importance']\n    \n    # Logistic Regression (use absolute coefficients averaged across classes)\n    if 'logistic_regression' in models:\n        lr_coefs = analyze_logistic_regression_coefficients(models['logistic_regression'], output_dir)\n        # Average absolute coefficients across classes\n        lr_importance = pd.DataFrame({\n            'Feature': lr_coefs[0]['Feature'],\n            'Importance': np.mean([np.abs(df['Coefficient'].values) for df in lr_coefs], axis=0)\n        }).sort_values('Importance', ascending=False)\n        all_importances['Logistic Regression'] = lr_importance.set_index('Feature')['Importance']\n    \n    # Neural Network\n    if 'neural_network' in models:\n        nn_importance = analyze_neural_network_permutation_importance(models['neural_network'], output_dir)\n        all_importances['Neural Network'] = nn_importance.set_index('Feature')['Importance_Mean']\n    \n    # Create comparison DataFrame\n    comparison_df = pd.DataFrame(all_importances)\n    comparison_df = comparison_df.fillna(0)\n    \n    # Normalize each column to 0-1 scale for comparison\n    comparison_df_norm = comparison_df.div(comparison_df.max(axis=0), axis=1)\n    \n    # Get top features across all models\n    top_features = comparison_df_norm.mean(axis=1).sort_values(ascending=False).head(20).index\n    \n    # Plot comparison\n    fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n    \n    # Normalized comparison\n    comparison_df_norm.loc[top_features].plot(kind='barh', ax=axes[0], width=0.8)\n    axes[0].set_xlabel('Normalized Importance', fontsize=12)\n    axes[0].set_title('Top 20 Features - Normalized Importance Comparison', fontsize=13, fontweight='bold')\n    axes[0].legend(fontsize=10)\n    axes[0].invert_yaxis()\n    axes[0].grid(True, alpha=0.3, axis='x')\n    \n    # Raw values\n    comparison_df.loc[top_features].plot(kind='barh', ax=axes[1], width=0.8)\n    axes[1].set_xlabel('Importance Score', fontsize=12)\n    axes[1].set_title('Top 20 Features - Raw Importance Comparison', fontsize=13, fontweight='bold')\n    axes[1].legend(fontsize=10)\n    axes[1].invert_yaxis()\n    axes[1].grid(True, alpha=0.3, axis='x')\n    \n    plt.tight_layout()\n    \n    output_path = Path(output_dir)\n    output_path.mkdir(parents=True, exist_ok=True)\n    plt.savefig(output_path / 'feature_importance_comparison.png', dpi=300, bbox_inches='tight')\n    print(f\"\\n\u2713 Saved: {output_path / 'feature_importance_comparison.png'}\")\n    plt.close()\n    \n    # Save comparison to CSV\n    comparison_df.to_csv(output_path / 'feature_importance_comparison.csv')\n    print(f\"\u2713 Saved: {output_path / 'feature_importance_comparison.csv'}\")\n    \n    return comparison_df\n\n\ndef interpret_feature_importance(comparison_df, output_dir='../reports'):\n    \"\"\"\n    Generate interpretation of feature importance.\n    \n    Args:\n        comparison_df: DataFrame with feature importances\n        output_dir: Directory to save report\n    \"\"\"\n    print(\"\\n\" + \"=\" * 60)\n    print(\"FEATURE IMPORTANCE INTERPRETATION\")\n    print(\"=\" * 60)\n    \n    # Get top features\n    avg_importance = comparison_df.mean(axis=1).sort_values(ascending=False)\n    top_features = avg_importance.head(10)\n    \n    print(\"\\nTop 10 Most Important Features (Average across models):\")\n    for i, (feature, importance) in enumerate(top_features.items(), 1):\n        print(f\"  {i:2d}. {feature:35s}: {importance:.4f}\")\n    \n    # Categorize features\n    rolling_features = [f for f in top_features.index if 'last5' in f]\n    context_features = [f for f in top_features.index if f in ['Arsenal_Home']]\n    opponent_features = [f for f in top_features.index if 'opponent' in f or 'historical' in f or 'previous' in f]\n    statistical_features = [f for f in top_features.index if any(x in f for x in ['strength', 'conversion', 'rate'])]\n    \n    print(\"\\nFeature Categories in Top 10:\")\n    print(f\"  Rolling Window Features: {len(rolling_features)}\")\n    print(f\"  Match Context Features: {len(context_features)}\")\n    print(f\"  Opponent Features: {len(opponent_features)}\")\n    print(f\"  Statistical Features: {len(statistical_features)}\")\n    \n    # Save interpretation\n    output_path = Path(output_dir)\n    output_path.mkdir(parents=True, exist_ok=True)\n    \n    with open(output_path / 'feature_importance_interpretation.txt', 'w') as f:\n        f.write(\"FEATURE IMPORTANCE INTERPRETATION\\n\")\n        f.write(\"=\" * 60 + \"\\n\\n\")\n        f.write(\"Top 10 Most Important Features:\\n\")\n        for i, (feature, importance) in enumerate(top_features.items(), 1):\n            f.write(f\"  {i:2d}. {feature:35s}: {importance:.4f}\\n\")\n        f.write(\"\\n\\nKey Insights:\\n\")\n        f.write(\"- Rolling window features (last 5 matches) are highly important\\n\")\n        f.write(\"- Recent form (goals, points, win rate) strongly predicts outcomes\\n\")\n        f.write(\"- Home advantage (Arsenal_Home) is a significant factor\\n\")\n        f.write(\"- Opponent strength and historical performance matter\\n\")\n    \n    print(f\"\\n\u2713 Saved interpretation: {output_path / 'feature_importance_interpretation.txt'}\")\n\n\nif __name__ == '__main__':\n    # Set style\n    plt.style.use('seaborn-v0_8')\n    sns.set_palette(\"husl\")\n    \n    # Load data and models\n    models, feature_cols = load_data_and_models()\n    \n    # Analyze each model type\n    comparison_df = compare_feature_importance_across_models(models)\n    \n    # Generate interpretation\n    interpret_feature_importance(comparison_df)\n    \n    print(\"\\n\" + \"=\" * 60)\n    print(\"FEATURE IMPORTANCE ANALYSIS COMPLETE\")\n    print(\"=\" * 60)\n    print(\"\\nAll visualizations saved to: ../figures/\")\n    print(\"Interpretation report saved to: ../reports/\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}