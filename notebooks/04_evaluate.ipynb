{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluate\n\nThis notebook contains the code from `evaluate.py`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\nModel Evaluation Module\n\nThis module evaluates all trained models and generates visualizations:\n- Confusion matrices\n- ROC curves (one-vs-rest for multi-class)\n- Classification reports\n- Performance comparison charts\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import (\n    confusion_matrix, classification_report, roc_curve, auc,\n    roc_auc_score, precision_recall_curve, average_precision_score\n)\nfrom sklearn.preprocessing import label_binarize\nfrom itertools import cycle\n\n# Model loading\nimport joblib\ntry:\n    import tensorflow as tf\n    from tensorflow import keras\n    DL_AVAILABLE = True\nexcept ImportError:\n    DL_AVAILABLE = False\n\n\ndef create_sequences_local(X, y, sequence_length):\n    \"\"\"\n    Create sequences from time series data (local copy to avoid import issues).\n    \"\"\"\n    X_seq = []\n    y_seq = []\n    \n    for i in range(sequence_length, len(X)):\n        X_seq.append(X[i-sequence_length:i])\n        y_seq.append(y.iloc[i] if hasattr(y, 'iloc') else y[i])\n    \n    return np.array(X_seq), np.array(y_seq)\n\n\ndef load_model_and_data(model_name, model_dir='../models', features_file='../data/processed/arsenal_features.csv'):\n    \"\"\"\n    Load a trained model and prepare test data.\n    \n    Args:\n        model_name: Name of the model ('logreg', 'random_forest', 'neural_network', etc.)\n        model_dir: Directory containing models\n        features_file: Path to features CSV\n        \n    Returns:\n        tuple: (model, X_test, y_test, predictions, probabilities, label_encoder)\n    \"\"\"\n    model_path = Path(model_dir)\n    features_path = Path(features_file)\n    \n    # Load features\n    df = pd.read_csv(features_path)\n    df['Date'] = pd.to_datetime(df['Date'])\n    df = df.sort_values('Date').reset_index(drop=True)\n    \n    # Separate features and target\n    feature_cols = [col for col in df.columns \n                   if col not in ['Result', 'Season', 'Date', 'Opponent']]\n    X = df[feature_cols].copy()\n    y = df['Result'].copy()\n    \n    # Handle missing values\n    X = X.fillna(X.median())\n    X = X.replace([np.inf, -np.inf], 0)\n    \n    # Test split (2024-25 season)\n    test_mask = df['Season'] == '2024-25'\n    X_test = X[test_mask].copy()\n    y_test = y[test_mask].copy()\n    \n    # IMPORTANT: Drop opponent_league_position if it exists (models were trained without it)\n    if 'opponent_league_position' in X_test.columns:\n        print(f\"  Note: Dropping 'opponent_league_position' (not in training data)\")\n        X_test = X_test.drop(columns=['opponent_league_position'])\n    \n    # Load model based on type\n    if model_name == 'logreg_baseline' or model_name == 'logreg':\n        model_data = joblib.load(model_path / 'logreg_baseline.pkl')\n        model = model_data['model']\n        scaler = model_data['scaler']\n        le = model_data['label_encoder']\n        \n        # Ensure feature alignment\n        X_test = X_test[scaler.feature_names_in_] if hasattr(scaler, 'feature_names_in_') else X_test\n        \n        X_test_scaled = scaler.transform(X_test)\n        y_test_encoded = le.transform(y_test)\n        y_pred = model.predict(X_test_scaled)\n        y_pred_proba = model.predict_proba(X_test_scaled)\n        \n    elif model_name == 'random_forest' or model_name == 'rf':\n        model_data = joblib.load(model_path / 'random_forest.pkl')\n        model = model_data['model']\n        le = model_data['label_encoder']\n        \n        # Ensure feature alignment\n        X_test = X_test[model.feature_names_in_] if hasattr(model, 'feature_names_in_') else X_test\n        \n        y_test_encoded = le.transform(y_test)\n        y_pred = model.predict(X_test)\n        y_pred_proba = model.predict_proba(X_test)\n        \n    elif model_name == 'neural_network' or model_name == 'nn':\n        if not DL_AVAILABLE:\n            return None\n        \n        model = keras.models.load_model(model_path / 'neural_network.h5')\n        preprocess_data = joblib.load(model_path / 'neural_network_preprocessing.pkl')\n        scaler = preprocess_data['scaler']\n        le = preprocess_data['label_encoder']\n        \n        # Ensure feature alignment\n        if hasattr(scaler, 'feature_names_in_'):\n            X_test = X_test[scaler.feature_names_in_]\n        \n        X_test_scaled = scaler.transform(X_test)\n        y_test_encoded = le.transform(y_test)\n        y_pred_proba = model.predict(X_test_scaled)\n        y_pred = np.argmax(y_pred_proba, axis=1)\n        \n    elif model_name == 'neural_network_weighted' or model_name == 'nn_weighted':\n        if not DL_AVAILABLE:\n            return None\n        \n        model = keras.models.load_model(model_path / 'neural_network_weighted.h5')\n        preprocess_data = joblib.load(model_path / 'neural_network_weighted_preprocessing.pkl')\n        scaler = preprocess_data['scaler']\n        le = preprocess_data['label_encoder']\n        \n        # Ensure feature alignment\n        if hasattr(scaler, 'feature_names_in_'):\n            X_test = X_test[scaler.feature_names_in_]\n        \n        X_test_scaled = scaler.transform(X_test)\n        y_test_encoded = le.transform(y_test)\n        y_pred_proba = model.predict(X_test_scaled)\n        y_pred = np.argmax(y_pred_proba, axis=1)\n        \n    elif model_name == 'lstm':\n        if not DL_AVAILABLE:\n            return None\n        \n        model = keras.models.load_model(model_path / 'lstm_model.h5')\n        preprocess_data = joblib.load(model_path / 'lstm_preprocessing.pkl')\n        scaler = preprocess_data['scaler']\n        le = preprocess_data['label_encoder']\n        sequence_length = preprocess_data['sequence_length']\n        \n        # Ensure feature alignment\n        if hasattr(scaler, 'feature_names_in_'):\n            X_test = X_test[scaler.feature_names_in_]\n        \n        # Create sequences\n        X_test_scaled = scaler.transform(X_test)\n        X_test_seq, y_test_seq = create_sequences_local(X_test_scaled, y_test, sequence_length)\n        y_test_encoded = le.transform(y_test_seq)\n        y_pred_proba = model.predict(X_test_seq)\n        y_pred = np.argmax(y_pred_proba, axis=1)\n        \n    elif model_name == 'hybrid_lstm':\n        if not DL_AVAILABLE:\n            return None\n        \n        model = keras.models.load_model(model_path / 'hybrid_lstm.h5')\n        preprocess_data = joblib.load(model_path / 'hybrid_lstm_preprocessing.pkl')\n        scaler = preprocess_data['scaler']\n        le = preprocess_data['label_encoder']\n        sequence_length = preprocess_data['sequence_length']\n        \n        # Ensure feature alignment\n        if hasattr(scaler, 'feature_names_in_'):\n            X_test = X_test[scaler.feature_names_in_]\n        \n        # Create sequences\n        X_test_scaled = scaler.transform(X_test)\n        X_test_seq, y_test_seq = create_sequences_local(X_test_scaled, y_test, sequence_length)\n        X_test_current = X_test_seq[:, -1, :]\n        y_test_encoded = le.transform(y_test_seq)\n        y_pred_proba = model.predict([X_test_seq, X_test_current])\n        y_pred = np.argmax(y_pred_proba, axis=1)\n        \n    else:\n        raise ValueError(f\"Unknown model name: {model_name}\")\n    \n    return {\n        'model': model,\n        'X_test': X_test,\n        'y_test': y_test,\n        'y_test_encoded': y_test_encoded,\n        'predictions': y_pred,\n        'probabilities': y_pred_proba,\n        'label_encoder': le\n    }\n\n\ndef plot_confusion_matrix(y_true, y_pred, class_names, model_name, output_dir='../figures'):\n    \"\"\"\n    Plot and save confusion matrix.\n    \n    Args:\n        y_true: True labels\n        y_pred: Predicted labels\n        class_names: List of class names\n        model_name: Name of the model\n        output_dir: Directory to save figure\n    \"\"\"\n    cm = confusion_matrix(y_true, y_pred)\n    \n    # Normalize confusion matrix\n    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n    \n    # Create figure with two subplots\n    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n    \n    # Raw counts\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n                xticklabels=class_names, yticklabels=class_names,\n                ax=axes[0], cbar_kws={'label': 'Count'})\n    axes[0].set_title(f'{model_name} - Confusion Matrix (Counts)', fontsize=14, fontweight='bold')\n    axes[0].set_ylabel('True Label', fontsize=12)\n    axes[0].set_xlabel('Predicted Label', fontsize=12)\n    \n    # Normalized percentages\n    sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Blues',\n                xticklabels=class_names, yticklabels=class_names,\n                ax=axes[1], cbar_kws={'label': 'Percentage'})\n    axes[1].set_title(f'{model_name} - Confusion Matrix (Normalized)', fontsize=14, fontweight='bold')\n    axes[1].set_ylabel('True Label', fontsize=12)\n    axes[1].set_xlabel('Predicted Label', fontsize=12)\n    \n    plt.tight_layout()\n    \n    # Save\n    output_path = Path(output_dir)\n    output_path.mkdir(parents=True, exist_ok=True)\n    plt.savefig(output_path / f'{model_name}_confusion_matrix.png', dpi=300, bbox_inches='tight')\n    print(f\"  \u2713 Saved confusion matrix: {output_path / f'{model_name}_confusion_matrix.png'}\")\n    plt.close()\n\n\ndef plot_roc_curves(y_true, y_pred_proba, class_names, model_name, output_dir='../figures'):\n    \"\"\"\n    Plot ROC curves for multi-class classification (one-vs-rest).\n    \n    Args:\n        y_true: True labels (encoded)\n        y_pred_proba: Predicted probabilities\n        class_names: List of class names\n        model_name: Name of the model\n        output_dir: Directory to save figure\n    \"\"\"\n    n_classes = len(class_names)\n    \n    # Binarize the output\n    y_true_bin = label_binarize(y_true, classes=range(n_classes))\n    \n    # Compute ROC curve and ROC area for each class\n    fpr = dict()\n    tpr = dict()\n    roc_auc = dict()\n    \n    for i in range(n_classes):\n        fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_pred_proba[:, i])\n        roc_auc[i] = auc(fpr[i], tpr[i])\n    \n    # Compute micro-average ROC curve and ROC area\n    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_true_bin.ravel(), y_pred_proba.ravel())\n    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n    \n    # Plot all ROC curves\n    plt.figure(figsize=(10, 8))\n    \n    colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'red', 'green'])\n    for i, color in zip(range(n_classes), colors):\n        plt.plot(fpr[i], tpr[i], color=color, lw=2,\n                label=f'{class_names[i]} (AUC = {roc_auc[i]:.2f})')\n    \n    # Micro-average\n    plt.plot(fpr[\"micro\"], tpr[\"micro\"], color='deeppink', linestyle='--', lw=2,\n            label=f'Micro-average (AUC = {roc_auc[\"micro\"]:.2f})')\n    \n    plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Random Classifier')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate', fontsize=12)\n    plt.ylabel('True Positive Rate', fontsize=12)\n    plt.title(f'{model_name} - ROC Curves (One-vs-Rest)', fontsize=14, fontweight='bold')\n    plt.legend(loc=\"lower right\", fontsize=10)\n    plt.grid(True, alpha=0.3)\n    \n    # Save\n    output_path = Path(output_dir)\n    output_path.mkdir(parents=True, exist_ok=True)\n    plt.savefig(output_path / f'{model_name}_roc_curves.png', dpi=300, bbox_inches='tight')\n    print(f\"  \u2713 Saved ROC curves: {output_path / f'{model_name}_roc_curves.png'}\")\n    plt.close()\n    \n    return roc_auc\n\n\ndef plot_precision_recall_curves(y_true, y_pred_proba, class_names, model_name, output_dir='../figures'):\n    \"\"\"\n    Plot Precision-Recall curves for each class.\n    \n    Args:\n        y_true: True labels (encoded)\n        y_pred_proba: Predicted probabilities\n        class_names: List of class names\n        model_name: Name of the model\n        output_dir: Directory to save figure\n    \"\"\"\n    n_classes = len(class_names)\n    \n    # Binarize the output\n    y_true_bin = label_binarize(y_true, classes=range(n_classes))\n    \n    # Compute Precision-Recall curve for each class\n    precision = dict()\n    recall = dict()\n    average_precision = dict()\n    \n    for i in range(n_classes):\n        precision[i], recall[i], _ = precision_recall_curve(y_true_bin[:, i], y_pred_proba[:, i])\n        average_precision[i] = average_precision_score(y_true_bin[:, i], y_pred_proba[:, i])\n    \n    # Plot\n    plt.figure(figsize=(10, 8))\n    \n    colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n    for i, color in zip(range(n_classes), colors):\n        plt.plot(recall[i], precision[i], color=color, lw=2,\n                label=f'{class_names[i]} (AP = {average_precision[i]:.2f})')\n    \n    plt.xlabel('Recall', fontsize=12)\n    plt.ylabel('Precision', fontsize=12)\n    plt.title(f'{model_name} - Precision-Recall Curves', fontsize=14, fontweight='bold')\n    plt.legend(loc=\"lower left\", fontsize=10)\n    plt.grid(True, alpha=0.3)\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    \n    # Save\n    output_path = Path(output_dir)\n    output_path.mkdir(parents=True, exist_ok=True)\n    plt.savefig(output_path / f'{model_name}_precision_recall.png', dpi=300, bbox_inches='tight')\n    print(f\"  \u2713 Saved Precision-Recall curves: {output_path / f'{model_name}_precision_recall.png'}\")\n    plt.close()\n    \n    return average_precision\n\n\ndef evaluate_model(model_name, model_dir='../models', output_dir='../figures', \n                   features_file='../data/processed/arsenal_features.csv'):\n    \"\"\"\n    Comprehensive evaluation of a single model.\n    \n    Args:\n        model_name: Name of the model to evaluate\n        model_dir: Directory containing models\n        output_dir: Directory to save visualizations\n        features_file: Path to features CSV\n    \"\"\"\n    print(f\"\\n{'='*60}\")\n    print(f\"EVALUATING: {model_name.upper()}\")\n    print(f\"{'='*60}\")\n    \n    try:\n        # Load model and data\n        results = load_model_and_data(model_name, model_dir, features_file)\n        if results is None:\n            print(f\"  \u2717 Could not load model: {model_name}\")\n            return None\n        \n        y_test = results['y_test']\n        y_test_encoded = results['y_test_encoded']\n        y_pred = results['predictions']\n        y_pred_proba = results['probabilities']\n        le = results['label_encoder']\n        \n        # Get class names\n        class_names = le.classes_\n        \n        # Classification report\n        print(f\"\\nClassification Report:\")\n        print(classification_report(y_test_encoded, y_pred, target_names=class_names))\n        \n        # Confusion matrix\n        print(f\"\\nGenerating confusion matrix...\")\n        plot_confusion_matrix(y_test_encoded, y_pred, class_names, model_name, output_dir)\n        \n        # ROC curves\n        print(f\"Generating ROC curves...\")\n        roc_auc = plot_roc_curves(y_test_encoded, y_pred_proba, class_names, model_name, output_dir)\n        print(f\"  ROC AUC scores:\")\n        for i, class_name in enumerate(class_names):\n            print(f\"    {class_name}: {roc_auc[i]:.4f}\")\n        print(f\"    Micro-average: {roc_auc['micro']:.4f}\")\n        \n        # Precision-Recall curves\n        print(f\"Generating Precision-Recall curves...\")\n        avg_precision = plot_precision_recall_curves(y_test_encoded, y_pred_proba, class_names, model_name, output_dir)\n        print(f\"  Average Precision scores:\")\n        for i, class_name in enumerate(class_names):\n            print(f\"    {class_name}: {avg_precision[i]:.4f}\")\n        \n        return {\n            'model_name': model_name,\n            'roc_auc': roc_auc,\n            'average_precision': avg_precision,\n            'classification_report': classification_report(y_test_encoded, y_pred, target_names=class_names, output_dict=True)\n        }\n        \n    except Exception as e:\n        print(f\"  \u2717 Error evaluating {model_name}: {e}\")\n        import traceback\n        traceback.print_exc()\n        return None\n\n\ndef compare_all_models(model_names, model_dir='../models', output_dir='../figures',\n                      features_file='../data/processed/arsenal_features.csv'):\n    \"\"\"\n    Evaluate and compare all models.\n    \n    Args:\n        model_names: List of model names to evaluate\n        model_dir: Directory containing models\n        output_dir: Directory to save visualizations\n        features_file: Path to features CSV\n    \"\"\"\n    print(\"=\"*60)\n    print(\"COMPREHENSIVE MODEL EVALUATION\")\n    print(\"=\"*60)\n    \n    all_results = []\n    \n    for model_name in model_names:\n        result = evaluate_model(model_name, model_dir, output_dir, features_file)\n        if result:\n            all_results.append(result)\n    \n    # Create comparison summary\n    if all_results:\n        print(f\"\\n{'='*60}\")\n        print(\"EVALUATION SUMMARY\")\n        print(f\"{'='*60}\")\n        \n        comparison_data = []\n        for result in all_results:\n            roc_auc = result['roc_auc']\n            comparison_data.append({\n                'Model': result['model_name'],\n                'ROC AUC (Win)': roc_auc.get(0, np.nan),\n                'ROC AUC (Draw)': roc_auc.get(1, np.nan),\n                'ROC AUC (Loss)': roc_auc.get(2, np.nan),\n                'ROC AUC (Micro-avg)': roc_auc.get('micro', np.nan),\n            })\n        \n        comparison_df = pd.DataFrame(comparison_data)\n        print(\"\\nROC AUC Comparison:\")\n        print(comparison_df.to_string(index=False))\n        \n        # Save comparison\n        output_path = Path(output_dir)\n        output_path.mkdir(parents=True, exist_ok=True)\n        comparison_df.to_csv(output_path / 'model_evaluation_comparison.csv', index=False)\n        print(f\"\\n\u2713 Saved evaluation comparison: {output_path / 'model_evaluation_comparison.csv'}\")\n    \n    return all_results\n\n\nif __name__ == '__main__':\n    # Set style\n    plt.style.use('seaborn-v0_8')\n    sns.set_palette(\"husl\")\n    \n    # Models to evaluate\n    models_to_evaluate = [\n        'logreg_baseline',\n        'random_forest',\n        'neural_network',\n        'neural_network_weighted',\n        'lstm',\n        'hybrid_lstm'\n    ]\n    \n    # Evaluate all models\n    results = compare_all_models(models_to_evaluate)\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"EVALUATION COMPLETE\")\n    print(\"=\"*60)\n    print(\"\\nAll visualizations saved to: ../figures/\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}